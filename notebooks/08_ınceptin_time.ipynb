{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/08_inception_time.ipynb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Veri Yolu Ayarları\n",
    "DATA_PATH = Path(\"d:/ecg/data/raw/ptbxl\")\n",
    "SAMPLING_RATE = 500  # PTB-XL'in örnekleme hızı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. InceptionTime Model Mimarisi\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, n_filters=32, kernel_sizes=[9, 19, 39], bottleneck_channels=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bottleneck = nn.Conv1d(in_channels, bottleneck_channels, 1) if bottleneck_channels else None\n",
    "        \n",
    "        # Paralel konvolüsyon katmanları\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                bottleneck_channels if bottleneck_channels else in_channels,\n",
    "                n_filters,\n",
    "                kernel_size,\n",
    "                padding=kernel_size // 2\n",
    "            ) for kernel_size in kernel_sizes\n",
    "        ])\n",
    "        \n",
    "        # MaxPool yolu\n",
    "        self.maxpool = nn.Sequential(\n",
    "            nn.MaxPool1d(3, stride=1, padding=1),\n",
    "            nn.Conv1d(in_channels, n_filters, 1)\n",
    "        )\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(n_filters * (len(kernel_sizes) + 1))\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.bottleneck:\n",
    "            x = self.bottleneck(x)\n",
    "        \n",
    "        # Paralel yolları birleştir\n",
    "        x_parallel = []\n",
    "        for conv in self.convs:\n",
    "            x_parallel.append(conv(x))\n",
    "        x_parallel.append(self.maxpool(x))\n",
    "        x = torch.cat(x_parallel, dim=1)\n",
    "        \n",
    "        return self.relu(self.bn(x))\n",
    "\n",
    "class InceptionTime(nn.Module):\n",
    "    def __init__(self, in_channels=12, n_classes=5, n_filters=32, n_blocks=6, kernel_sizes=[9, 19, 39]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[\n",
    "            InceptionBlock(\n",
    "                in_channels if i == 0 else n_filters * 4,\n",
    "                n_filters=n_filters,\n",
    "                kernel_sizes=kernel_sizes\n",
    "            ) for i in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(n_filters * 4, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        x = self.gap(x).squeeze(-1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Veri yükleniyor...\n",
      "Veri yolu kontrolü: d:\\ecg\\data\\raw\\ptbxl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kayıtlar yükleniyor:   1%|          | 147/21837 [00:04<07:08, 50.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hata (kayıt 136): Header dosyası bulunamadı: d:\\ecg\\data\\raw\\ptbxl\\records500\\00000\\00137_hr.hea\n",
      "Dosya yolu: d:\\ecg\\data\\raw\\ptbxl\\records500\\00000\\00137_hr\n",
      "\n",
      "Hata (kayıt 138): Header dosyası bulunamadı: d:\\ecg\\data\\raw\\ptbxl\\records500\\00000\\00139_hr.hea\n",
      "Dosya yolu: d:\\ecg\\data\\raw\\ptbxl\\records500\\00000\\00139_hr\n",
      "\n",
      "Hata (kayıt 139): Header dosyası bulunamadı: d:\\ecg\\data\\raw\\ptbxl\\records500\\00000\\00140_hr.hea\n",
      "Dosya yolu: d:\\ecg\\data\\raw\\ptbxl\\records500\\00000\\00140_hr\n",
      "\n",
      "Hata (kayıt 140): Header dosyası bulunamadı: d:\\ecg\\data\\raw\\ptbxl\\records500\\00000\\00141_hr.hea\n",
      "Dosya yolu: d:\\ecg\\data\\raw\\ptbxl\\records500\\00000\\00141_hr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kayıtlar yükleniyor: 100%|██████████| 21837/21837 [13:12<00:00, 27.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Toplam 38 kayıt yüklenemedi\n",
      "\n",
      "Başarıyla yüklenen kayıt sayısı: 21799\n",
      "\n",
      "Veri yükleme başarılı!\n",
      "X şekli: (21799, 12, 5000)\n",
      "y şekli: (21837, 5)\n",
      "Sınıflar: ['NORM', 'MI', 'STTC', 'CD', 'HYP']\n"
     ]
    }
   ],
   "source": [
    "# 3. Veri Yükleme ve Ön İşleme\n",
    "def load_ptbxl_data(data_path, sampling_rate=500):\n",
    "    \"\"\"PTB-XL veri setini yükle\"\"\"\n",
    "    \n",
    "    print(f\"Veri yolu kontrolü: {data_path}\")\n",
    "    \n",
    "    # Meta veriyi oku\n",
    "    meta_path = data_path / \"ptbxl_database.csv\"\n",
    "    if not meta_path.exists():\n",
    "        raise FileNotFoundError(f\"Meta veri dosyası bulunamadı: {meta_path}\")\n",
    "        \n",
    "    df = pd.read_csv(meta_path)\n",
    "    df.scp_codes = df.scp_codes.apply(eval)\n",
    "    \n",
    "    # Tanı sınıfları\n",
    "    diagnostic_classes = ['NORM', 'MI', 'STTC', 'CD', 'HYP']\n",
    "    \n",
    "    # Etiketleri hazırla\n",
    "    labels = np.zeros((len(df), len(diagnostic_classes)))\n",
    "    for idx, row in df.iterrows():\n",
    "        for diagnosis in row.scp_codes:\n",
    "            if diagnosis in diagnostic_classes:\n",
    "                labels[idx, diagnostic_classes.index(diagnosis)] = 1\n",
    "    \n",
    "    # Veri yükleme fonksiyonu\n",
    "    def load_waveform(path):\n",
    "        if not path.parent.exists():\n",
    "            raise FileNotFoundError(f\"Dizin bulunamadı: {path.parent}\")\n",
    "        if not (path.parent / f\"{path.name}.hea\").exists():\n",
    "            raise FileNotFoundError(f\"Header dosyası bulunamadı: {path}.hea\")\n",
    "            \n",
    "        record = wfdb.rdrecord(str(path))\n",
    "        return record.p_signal.T  # (leads, time_steps) formatına çevir\n",
    "    \n",
    "    # Tüm kayıtları yükle\n",
    "    waveforms = []\n",
    "    errors = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Kayıtlar yükleniyor\"):\n",
    "        try:\n",
    "            # EKG kayıt yolu düzeltildi\n",
    "            record_path = data_path / row['filename_hr'].replace('.hea', '')\n",
    "            waveform = load_waveform(record_path)\n",
    "            \n",
    "            # Normalize et\n",
    "            scaler = StandardScaler()\n",
    "            waveform = scaler.fit_transform(waveform.T).T\n",
    "            waveforms.append(waveform)\n",
    "            \n",
    "        except Exception as e:\n",
    "            errors.append((idx, str(e)))\n",
    "            if len(errors) < 5:  # İlk 5 hatayı göster\n",
    "                print(f\"\\nHata (kayıt {idx}): {str(e)}\")\n",
    "                print(f\"Dosya yolu: {record_path}\")\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"\\nToplam {len(errors)} kayıt yüklenemedi\")\n",
    "    \n",
    "    if not waveforms:\n",
    "        raise ValueError(\"Hiç kayıt yüklenemedi!\")\n",
    "        \n",
    "    print(f\"\\nBaşarıyla yüklenen kayıt sayısı: {len(waveforms)}\")\n",
    "    \n",
    "    return np.array(waveforms), labels, diagnostic_classes\n",
    "\n",
    "# Test et\n",
    "try:\n",
    "    print(\"\\nVeri yükleniyor...\")\n",
    "    X, y, class_names = load_ptbxl_data(DATA_PATH)\n",
    "    print(\"\\nVeri yükleme başarılı!\")\n",
    "    print(f\"X şekli: {X.shape}\")\n",
    "    print(f\"y şekli: {y.shape}\")\n",
    "    print(f\"Sınıflar: {class_names}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nHata: {str(e)}\")\n",
    "    \n",
    "    # Dizin yapısını kontrol et\n",
    "    print(\"\\nDizin yapısı:\")\n",
    "    if DATA_PATH.exists():\n",
    "        print(f\"\\n{DATA_PATH} içeriği:\")\n",
    "        for item in DATA_PATH.glob(\"*\"):\n",
    "            print(f\"- {item.name}\")\n",
    "            if item.is_dir():\n",
    "                for subitem in item.glob(\"*\"):\n",
    "                    print(f\"  - {subitem.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Eğitim Fonksiyonu\n",
    "def train_model(model, train_loader, test_loader, n_epochs=50):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    history = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}')):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Accuracy\n",
    "            pred = torch.sigmoid(output) > 0.5\n",
    "            train_acc += (pred == target).float().mean().item()\n",
    "            \n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc /= len(train_loader)\n",
    "        \n",
    "        # Testing\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                test_loss += criterion(output, target).item()\n",
    "                \n",
    "                pred = torch.sigmoid(output) > 0.5\n",
    "                test_acc += (pred == target).float().mean().item()\n",
    "        \n",
    "        test_loss /= len(test_loader)\n",
    "        test_acc /= len(test_loader)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(test_loss)\n",
    "        \n",
    "        # History\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        \n",
    "        print(f'\\nEpoch {epoch+1}/{n_epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_loss,\n",
    "            }, 'best_inception_model.pt')\n",
    "    \n",
    "    return history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
